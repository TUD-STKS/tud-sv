{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "# 2.2 - Linear Predictive Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<img style=\"float: right; margin:5px 0px 0px 10px\" src=\"img/2-title.png\" width=\"500\">\n",
    "Das \"Linear Predictive Coding\" (LPC) ist ein Verfahren, das hauptsächlich in der Audiosignalverarbeitung und Sprachverarbeitung verwendet wird, um die Spektralhüllkurve eines digitalen Sprachsignals in komprimierter Form unter Verwendung der Informationen eines linearen Vorhersagemodells darzustellen. Es ist eine der leistungsstärksten Sprachanalysetechniken und eine der nützlichsten Methoden zum Codieren von Sprache in guter Qualität mit einer niedrigen Bitrate und liefert hochgenaue Schätzungen von Sprachparametern. LPC ist die am weitesten verbreitete Methode in der Sprachcodierung und Sprachsynthese.\n",
    "\n",
    "Thematisch wird LPC erst in Thema5-Hüllkurven behandelt, da sich mit LPC aber im Zuge der Bestimmung der Einhüllenden auch die Systemfunktion des z.B. Vokaltraktes (d.h. des dazugehörige _Synthesfilters_) schätzen lassen, wird sie in den Notebooks an dieser Stelle eingeführt. LPC unterscheidet sich aber grundlegend von dem Vorgehen der Messung der Übertragungsfunktion, wie es am Beispiel des Sweeps im Notebook 2.1 behandelt wurde.\n",
    "\n",
    "\n",
    "## Inhalt  \n",
    "<table style=\"width:256px; border: 1px solid black; display: inline-block\">\n",
    "    <tr>\n",
    "        <td style=\"text-align:right\"><img src=\"img/2-1.png\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=128px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#1'>\n",
    "                1. Grundlagen\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td  style=\"text-align:right\" width=64px><img src=\"img/2-2.jpg\" style=\"float:left\"></td>\n",
    "        <td style=\"text-align:left\" width=256px>\n",
    "            <a style=\"color:black; font-size:12px; font-weight:bold; text-decoration:none\" href='#2'>\n",
    "                2. Anwendungsbeispiel\n",
    "            </a>\n",
    "        </td>\n",
    "    </tr>  \n",
    "</table>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<a id='1'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-1.png\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        1. Grundlagen\n",
    "    </h2>\n",
    "</div>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/Quelle-Filter-Modell.jpg\" width=\"800\">\n",
    "\n",
    "Die artikulatorische Sprachsynthese (anders als z.B. die datengestützte Sprachsynthese, wie sie in sämtlichen Sprachassistenten angewendet wird) basiert auf der sog. Quelle-Filter-Theorie, welche davon ausgeht, dass die Sprachproduktion in zwei Prozesse unterteilt werden kann, die unabhängig voneinander sind: die Erzeugung eines Anregungssignales und die anschließende Filterung dieses Anregungssignales. Unabhängig bedeutet in diesem Kontext, dass der nachgeschaltete Filterprozess das Anregungssignal nicht verändert, also keine Rückkopplung entsteht.\n",
    "\n",
    "Vereinfacht dargestellt erzeugen bei der Sprachproduktion die Glottis und die Stimmlippen unter Anregung eines Luftstromes das Anregungssignal, welches vom daraufffolgenden Vokaltrakt gefiltert wird. Das Anregungssignal kann dabei stimmhaft sein (d.h. es besitzt eine Grundperiode), stimmlos (Rauschrignal) oder eine Mischung aus Beidem. Wenn sich die Stimmlippen in einem vokalisierten Zustand befinden (Vibration der Stimmlippen), werden stimmhafte Laute (zum Beispiel Vokale) erzeugt. Wenn sich die Stimmbänder in einem \"stillen\" Zustand befinden, werden stimmlose Laute (zum Beispiel Konsonanten) erzeugt. \n",
    "\n",
    "LPC bietet die Möglichkeit, die Systemfunktion des Vokaltraktes zu schätzen, ohne das Anregungssignal zu kennen.\n",
    "\n",
    "#### Grundidee:   \n",
    "Die Grundidee der LPC ist zunächst generell, dass die einzelnen Abtastwerte eines linear gefilerten Ausgangssignals $y(k)$ (in Falle der Sprachproduktion das abgestrahle Sprachschallsignal) nicht unabhängig voneinander sind. Stattdessen lässt sich jeder Abtastwert $y(k)$ aus einer Linearkombination endlich vielen vorangegangenen Abtastwerten annähern:\n",
    "\\begin{equation}\n",
    "\\hat{y}(k)=\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "Die Gewichtungsfaktoren $a_i$ heißen _Prädiktorkoeffizienten_.\n",
    "\n",
    "Für jeden Abtastwert wird damit ein gewisser Prädiktionsfehler $e(k)$ gemacht:\n",
    "\\begin{equation}\n",
    "e(k)=y(k)-\\hat{y}(k)=y(k)-\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die $a_{i}$ werden geschätzt, indem der mittlere quadratische Fehler (der sog. MSE, _mean squared error_) im betrachteten Signalabschnitt minimiert wird. \n",
    "\n",
    "Die Gleichung für $y(k)$ entspricht formal der Rekursionsgleichung für einen IIR-Allpol-Filter $H(z)$, wobei das Fehlersignal als (mit der Verstärkung $G$ skaliertes) Eingangssignal $x(k)$ betrachtet werden kann:\n",
    "\\begin{equation}\n",
    "y(k)=e(k)+\\sum_{i=1}^{N}a_{i}y(k-i)=Gx(k)+\\sum_{i=1}^{N}a_{i}y(k-i)\n",
    "\\end{equation}\n",
    "\n",
    "Die Systemfunktion ist somit\n",
    "\\begin{equation}\n",
    "H(z)=\\frac{Y(z)}{E(z)}=\\frac{1}{1-\\sum_{i=1}^{N}a_{i}z^{-i}}\n",
    "\\end{equation}\n",
    "\n",
    "Dieser Filter ist der sogenannte __LPC-Synthesefilter__. Der Amplitudengang des Synthesefilters entspricht bei einem weißen Anregungsspektrum der\n",
    "Hüllkurve im Frequenzbereich.\n",
    "\n",
    "Umgekehrt lässt sich das Fehlersignal $e(k)$ durch die Filterung von $y(k)$ mit dem inversen Filter $A(z) = 1/H(z)$ bestimmen. Dieser Filter wird als __Analysefilter__ bezeichnet.\n",
    "\n",
    "<img style=\"float:top; margin:5px 0px 0px 10px\" src=\"img/lpc.png\" width=\"800\">\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "<a id='2'></a>\n",
    "<div>\n",
    "    <img src=\"img/2-2.jpg\" style=\"float:left\">\n",
    "    <h2 style=\"position: relative; top: 6px; left: 6px\">\n",
    "        2.  Anwendungsbeispiel\n",
    "    </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "LPC findet, wie bereits erwähnt, breite Anwendung in der Spracherkennung, Sprachsynthese und Sprachkompression. Als Beispiel soll hier die lineare Prädiktion zur Formanterkennung verwendet werden, d.h., um die Formantfrequenzen (Resonanzfrequenzen) des Vokaltrakts zu schätzen. Im Quelle-Filter Sprachsignalmodell entsprechen die Formantfrequenzen den komplexen Polpaaren der Übertragungsfunktion. \n",
    "\n",
    "Ausgehend von einem kurzen Abschnitt des Signals werden die Prädiktorkoeffizienten geschätzt, welche wiederum die Filterkoeffizienten $a_i$ des Nennerpolynoms darstellen. Jede komplexe Wurzel der diskreten Systemfunktion des Synthesefilters lässt sich bekanntermaßen auch über \n",
    "\\begin{equation}\n",
    "p_{i}=r_{i}e^{j \\Omega_{i}},\\qquad r_i=\\sqrt{real(p_i)^2+imag(p_i)^2},\\qquad \\Omega_i = atan\\left(\\frac{imag(p_i)}{real(p_i)}\\right)\n",
    "\\end{equation}\n",
    "darstellen, d.h. die geschätzten Formantfrequenzen $f_i$ sind \n",
    "\\begin{equation}\n",
    "f_i = \\frac{\\Omega_i\\cdot f_s}{2\\pi},\n",
    "\\end{equation}\n",
    "Wobei nur die Frequenzen mit  positivem Vorzeichen in Frage kommen. Außerdem müssen die Formantfrequenzen überhalb der Grundfrequenz liegen, d.h. eine zusätzliche Bedingung ist $f_i >\\approx$ 90-100 Hz.\n",
    "\n",
    "Importieren Sie zuerst die verwendeten externen Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Import externer Module\n",
    "'''\n",
    "# ToDo: Importieren Sie:\n",
    "#    - numpy mit dem Alias np,\n",
    "#    - Das Modul IPython.display mit dem Alias ipd\n",
    "#    - Das Modul pyplot aus der Bibliothek matplotlib mit dem Alias plt,\n",
    "#    - scipy\n",
    "#    - Das Modul wavfile aus der Bibliothek scipy.io\n",
    "#    - librosa\n",
    "#    - Das Modul interact_manual aus der Bibliothek ipywidgets in den globalen Namenraum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Nun laden Sie das Audiosignal `akustik.wav` in das Projekt und bestimmen Sie dessen Eigenschaften:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audiosignal laden\n",
    "'''\n",
    "\n",
    "fs_Hz, audioSignal =  [..]          # ToDo: Laden Sie 'akustik.wav' mittels wavfile.read() in das Projekt\n",
    "audioSignal = [..]                  # ToDo: Normalisierung von audioSignal\n",
    "signalLength = [..]                 # ToDo: Länge des Audiosignals\n",
    "T_s = (signalLength-1) / fs_Hz      # Zeit\n",
    "t_s = [..]                          # ToDo: Erzeugen Sie ein Array mit den Abtastzeitpunkten mittels np.linspace() \n",
    "f_Hz = [..]                         # ToDo: Erzeugen Sie ein Array mit den Frequenzpunkten von [0-fs_Hz/2] mittels np.linspace()\n",
    "\n",
    "print(\"Abtastrate : %d Hz, Signaldauer : %.2f s\" %(fs_Hz, T_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Hören Sie sich nun das Array `audioSignal` an. Erstellen Sie dafür eine Funktion \"play_audio\", die mittels `ipd.Audio` Zahlenarrays als Signal über die Lautsprecher ausgibt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion play_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das Eingangsarray via ipd.Audio ausgibt.\n",
    "def play_audio(acoustic_signal_i, fs_Hz):\n",
    "    [..]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"play_audio\" an, um sich \"audioSignal\" anzuhören."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Zur Bestimmung der Segmentlänge ist ein Kompromiss zwischen zwei gewünschten Eigenschaften zu finden:\n",
    " 1. Das Segment soll so klein wie möglich sein, um das Spektrum an einem Zeitpunkt zu erhalten,\n",
    " 2. Das Segment soll so groß wie möglich sein, um das Spektrum invariant gegenüber der Lage der Gtrundperiode zu machen. \n",
    " \n",
    "Die Grundperiode des Signals liegt zwischen 8-12 ms. Als guten Kompromiss zwischen den beiden Eigenschaften hat sich die Segmentlänge von 32 ms erwiesen. Das hat auch den Vorteil, dass diese Länge bei einer Abtastrate von 16 kHz insgesamt 512 Elemente umfasst, welches eine optimale Größe bei der Anwendung der FFT bedeutet.\n",
    "\n",
    "Schneiden Sie das Signal nun in ein 32 ms bzw. 512 Elemente langes Segment, in dem der Vokal /a/ gesagt wird (Startpunkt zum Beispiel bei 1,18 s), und berechnen Sie das Spektrum des /a/-Vokaltrakt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 32 ms langen /a/-Segments des Wortes „Akustik“\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "n_start =     [..]      # ToDo: Bestimmen Sie die Startsample-Nummer\n",
    "n_end =    [..]         # ToDo: Bestimmen Sie die Endsample-Nummer\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_aSegment_s = [..]  # ToDo: Segmentieren Sie das Zeitarray t_s auf das oben bestimmte Zeitintervall\n",
    "a_segment =  [..]  # ToDo: Segmentieren Sie das Signalarray audioSignal auf das oben bestimmte Zeitintervall               \n",
    "\n",
    "# Spektrum berechnen\n",
    "n_fft = 512  # Länge von FFT\n",
    "f_aSegment_Hz = np.linspace(0, fs_Hz/2, int(n_fft/2))   # Frequenzbereich\n",
    "aSegment_fft = [..]  # ToDo: wenden Sie eine FFT an mittels scipy.fftpack.fft()\n",
    "aSegment_fft_plot = np.abs(aSegment_fft[:len(f_aSegment_Hz)]) / int(n_fft/2)\n",
    "\n",
    "# Graphische Darstellung \n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal eines /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(aSegment_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Die FFT geht von einer periodischen Fortführung des Signals aus. Beim Segmentieren des Signals entstehen dadurch Sprünge zwischen dem ersten und letzten Wert, die hochfrequente Anteile erzeugen, die im Signal eigentlich nicht vorhanden sind. Deshalb sollte das Signal vor der Verarbeitung gefenstert werden. Verwenden Sie dafür ein \"Von-Hann-Fenster\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Anwendung des von-Hann-Fensters auf das /a/-Segment\n",
    "'''\n",
    "\n",
    "# Audiosegment fenstern\n",
    "a_segment_win = [..]    # ToDo: Fenstern des Arrays a_segment mit \"signal.hann()\"\n",
    "\n",
    "# Spektrum des von-Hann-gefensterten Signals\n",
    "aSegment_fft_win = [..]  # ToDo: wenden Sie eine FFT an mittels scipy.fftpack.fft()\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Audiosignal des gefensterten /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_aSegment_s, a_segment_win)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Amplitudenfrequenzgang des gefensterten /a/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_aSegment_Hz, np.log(np.abs(aSegment_fft_win[:len(f_aSegment_Hz)])))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Geben Sie sich nun das segmentierte Audiosignal `a_segment` aus. Da das Signal nur 32 ms lang ist, muss das Signal wiederholt werden. Definieren Sie deshalb dafür eine Funktion \"repeat_audio\", die das eingegebene Signal mehrfach hintereinander hängt (erwarten sie dabei keine schön klingendes Signal. Es soll lediglich demonstriert werden, dass eine Tendenz hörbar ist, um welchen Vokal es sich handelt, selbst wenn eine sehr simple Verkettung der Einzelsegmente vorgenommen wird)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion repeat_audio\n",
    "'''\n",
    "# ToDo: Erstellen sie eine Funktion, die das eingegebene Array mehrfach hintereinanderhängen.\n",
    "def repeat_audio( [..] ):\n",
    "    [..]\n",
    "    for i in range(0, 7):\n",
    "        [..] = np.concatenate(( [..] ), axis = None)   # ToDo: eine Möglichkeit zum hintereinanderhängen des Signals\n",
    "    \n",
    "    # Optional: Audioausgabe mittels Boolscher Variable steuerbar \n",
    "    if [..] :\n",
    "        [..]\n",
    "        \n",
    "    return [..]  # zusammengefügtes Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    " Hören Sie Sich sowohl das unbearbeitete als auch das gefensterte Segment an: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "a_segment_rep = [..] # ToDo: wenden Sie die Funktionen \"repeat_audio\" und \"play_audio\" an, um sich \"a_segment\" und \"a_segment_win\" anzuhören.\n",
    "a_segment_win_rep = [..]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Das bloße wiederholte Abspielen des Segments sorgt für eine schlechte Audioausgabe. Um dies etwas zu verbessern, kann das Signal überlappt hintereinander abgespielt werden (dies wird nochmal wichtiger beim Abspielen des synthetisierten Signals). Die folgende Funktion \"overlap_audio\" tut genau das. Als Eingabe wird das Segment und eine Wiederholzeit angegeben. Das Signal wird ausgegeben und zudem die berechnete Überlappung der Segmente zurückgegeben. Wenden Sie die Funktion auf das /a/-Segment an mit einer Wiederholdauer von 10 ms. Hören Sie sich das Signal an und schauen Sie sich einen Ausschnitt der entstandenen Audiospur an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Definition der Funktion overlap_audio\n",
    "'''\n",
    "def overlap_audio(signal_segment_i, t_overlap_ms_i, signal_output = False):\n",
    "    n_repetition = int(0.001 * t_overlap_ms_i * 16000)   # Berechnung der Wiederholdauer in Abtastpunkte\n",
    "    \n",
    "    playable_audio = np.zeros(512)                       # Initialisierung\n",
    "    for i in range(0, 200):                              # Überlappung von 200 Segmenten\n",
    "        playable_audio[i*n_repetition:] = playable_audio[i*n_repetition:] + signal_segment_i\n",
    "        playable_audio = np.concatenate((playable_audio, np.zeros(n_repetition)))\n",
    "    playable_audio = playable_audio[190:-512]            # Schneiden der Unüberlappten Stellen (vorne und hinten)\n",
    "    \n",
    "    # Ausgabe\n",
    "    if signal_output == True:\n",
    "        sound = (playable_audio*(2**15-1)/np.max(np.abs(playable_audio))).astype(np.int16)\n",
    "        play_obj = sa.play_buffer(sound, 1, 2, int(fs_Hz))\n",
    "        play_obj.wait_done()\n",
    "    return playable_audio                                # Rückgabe des berechneten Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe und graphische Darstellung\n",
    "'''\n",
    "\n",
    "\n",
    "T_overlap_ms = [..]         # ToDo: Geben Sie den richtigen Wert ein\n",
    "\n",
    "playable_audio_a = [..]     # ToDo: Wenden Sie die Funktion \"overlap_audio\" auf a_segment an\n",
    "playable_audio_a_win = [..] # ToDo: Wenden Sie die Funktion \"overlap_audio\" auf a_segment_win an\n",
    "\n",
    "t_range_s = np.linspace(0, (1024-1)/fs_Hz, 1024)\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('addiertes Audiosignal des /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_range_s, playable_audio_a[:1024])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('addiertes Audiosignal des gefensterten /a/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_range_s, playable_audio_a_win[:1024])\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "Ähnlich können Sie das Spektrum eines 32 ms langen /u/-Segments des Wortes „Akustik“ berechnen (Anfangspunkt zum Beispiel: 1,42 s). Berechnen Sie das Spektrum auch für das Von-Hann-gefensterte Segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spektrum eines 32 ms langen /u/-Segments des Wortes „Akustik“ berechnen\n",
    "''' \n",
    "# Zeitintervall bestimmen\n",
    "[..]\n",
    "\n",
    "# Audiosignal segmentieren\n",
    "t_uSegment_s = [..]\n",
    "u_segment = [..]\n",
    "\n",
    "# Fenstern\n",
    "u_segment_win = [..]\n",
    "\n",
    "# Spektren berechnen\n",
    "uSegment_fft_plot = [..]\n",
    "uSegment_win_fft_plot = [..]\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(221)\n",
    "plt.title('Audiosignal des /u/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Amplitudenfrequenzgang des /u/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_fft_plot))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('Audiosignal des gefensterten /u/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_uSegment_s, u_segment_win)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('Amplitudenfrequenzgang des gefensterten /u/-Segments')\n",
    "plt.xlabel('Frequenz [Hz]') \n",
    "plt.ylabel('log|X(f)|') \n",
    "plt.plot(f_uSegment_Hz, np.log(uSegment_win_fft_plot))\n",
    "\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Geben Sie sich nun auch das segmentierte Signal `u_segment` und das gefensterte Signal `u_segment_win` mit den Funktionen \"repeat_audio\" und \"overlap_audio\" (10 ms-Wiederholdauer) aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe repeat\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich \"u_segment\" anzuhören"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe overlap\n",
    "'''\n",
    "T_overlap_ms = [..]\n",
    "\n",
    "playable_audio_u = [..] # ToDo: wenden Sie die Funktion \"overlap_audio\" an, um sich \"u_segment\" und \"u_segment_win\" anzuhören\n",
    "playable_audio_u_win = [..] \n",
    "\n",
    "\n",
    "t_range_s = np.linspace(0, (1024-1)/fs_Hz, 1024)\n",
    "\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('addiertes Audiosignal des /u/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_range_s, playable_audio_u[:1024])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('addiertes Audiosignal des gefensterten /u/-Segments')\n",
    "plt.xlabel('Zeit [s]') \n",
    "plt.ylabel('x(t)') \n",
    "plt.plot(t_range_s, playable_audio_u_win[:1024])\n",
    "\n",
    "plt.gcf().set_size_inches(15, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Jetzt können wir LP-Koeffizienten berechnen, um die Pole der Systemfunktion des Synthesefilters zu bestimmen. Dafür verwenden wir nun direkt eine Funktion des Moduls `librosa`, der LP-Koeffizienten sucht: [librosa.lpc(y, order)](https://librosa.org/doc/latest/generated/librosa.lpc.html)\n",
    "\n",
    "Dieser soll zunächst für das ungefensterte /a/-Segment \"a_segment\" mit einer Filterordnung der Größe N = 12 betrachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Die LP-Koeffizienten des /a/-Segments berechnen und die Frequenzgang bzw. Polverteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "N = [..]           # ToDo: Legen Sie die Modellordnung des Synthesefilters fest\n",
    "b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.subplot(121)\n",
    "plt.title('Amplitudenfrequenzgang des Synthesefilters für /a/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Frequenz [Hz]')\n",
    "plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Image')\n",
    "theta = np.arange(0, 2*np.pi, 0.01)\n",
    "plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "plt.plot(np.real(p), np.imag(p), 'x')\n",
    "plt.axis(\"equal\")\n",
    "\n",
    "plt.gcf().set_size_inches(15, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Kritisch ist dabei die Wahl der „richtigen“ Modellordnung N. Die Unterschiede im Frequenzgang bei der Wahl unterschiedlicher Modellgrößern, soll nun für das /u/-Vokal-Segment genauer betrachtet werden. Erzeugen Sie dafür mittels for-Schleife die Amplitudenfrequenzgänge bei Wahl der Synthesesfilterordnung auf 4., 8., 12., 20. und 24. Ordnung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Aufgabe: Die LP-Koeffizienten des /u/-Segments mit verschiedenen Ordnungen berechnen und\n",
    "          die Frequenzgang bzw. Null-Pol Verteilung des Synthesefilters aufzeichnen\n",
    "'''\n",
    "for N in ( [..] ):     # ToDo: Vorgabe der Filterordnungsgrößen  \n",
    "    b, a = [1], [..]   # ToDo: Bestimmen Sie die LP-Koeffizienten des Nenners über librosa.lpc()\n",
    "    fH_Hz, H = [..]    # ToDo: Erzeugen Sie den Amplitudenfrequenzgang mit scipy.signal.freqz()\n",
    "    z, p, k = [..]     # ToDo: Erzeugen Sie die Null-Pol Verteilung mi scipy.signal.tf2zpk()\n",
    "\n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(121)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    \n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Um sich nun den Synthesefilter anzuhören, kann dieser invers Fouriertransformiert werden, um daraus die Impulsantwort zu erhalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Impulsantwort des modellierten /u/-Vokalfilters mit der Ordnung N = 24\n",
    "'''\n",
    "# Impulsantwort h des Modellfilters H\n",
    "h = [..]    # ToDo: implementieren Sie eine IFFT von H mittels scipy.fftpack.ifft()\n",
    "t_syn_s = np.linspace(0, (len(H)-1)/fs_Hz, len(H))\n",
    "\n",
    "# Graphische Darstellung\n",
    "plt.title('Impulsantwort des Synthesefilters für /u/ mit Ordnung=%d' %N)\n",
    "plt.xlabel('Zeit [s]')\n",
    "plt.ylabel('h [t]')\n",
    "\n",
    "plt.plot(t_syn_s, np.real(h[int(len(H)):]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Erzeugen Sie nun ein Audiosignal, indem Sie die ersten 10 ms bzw. ~170 Samples (genauer [512:512+170]) mittels der \"repeat_audio\" abspielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Audioausgabe\n",
    "'''\n",
    "[..] # ToDo: wenden Sie die Funktion \"repeat_audio\" an, um sich die Impulsantwort \"h[512:512+170]\" anzuhören."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Zum Schluss soll nun mittels `interact_manual` Änderungen in der Modellordnung  betrachtet werden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Interaktive LPC-Darstellung\n",
    "'''\n",
    "\n",
    "@interact_manual(N_i=(2, 50, 2),  segment_type_i=[('a-Segment', 1), ('u-Segment', 2)], output_sound_i = True, output_type_i=[('real', 1), ('real+win', 2),('real+win+overlap', 3),('syn+uncut', 4), ('syn+overlap', 5)], t_overlap_ms_i = (1, 15, 1))\n",
    "def interactive_linear_sweep(N_i=10, segment_type_i = 'a-Segment', output_sound_i=True, output_type_i = 'win+added', t_overlap_ms_i = 10):\n",
    "    if segment_type_i == 1:\n",
    "        segment = a_segment\n",
    "    elif segment_type_i == 2:\n",
    "        segment = u_segment\n",
    "    \n",
    "    # Berechnung der LP-Koeffizienten\n",
    "    b, a = [..]\n",
    "    \n",
    "    # Bestimmung des Amplitudenfrequenzgang und der Pol-Nullstellen-Verteilung\n",
    "    fH_Hz, H = [..]\n",
    "    z, p, k = [..]\n",
    "    \n",
    "    # Impulsantwort h des Modellfilters H\n",
    "    h = [..]\n",
    "    t_s = [..]\n",
    "    \n",
    "    # Audioauswahl\n",
    "    if output_type_i == 1:  # real + uncut\n",
    "        chosen_audio = repeat_audio( [..] )          \n",
    "    elif output_type_i == 2: # real + windowed\n",
    "        chosen_audio = repeat_audio( [..] )\n",
    "    elif output_type_i == 3: # real + windowed + overlap\n",
    "        chosen_audio = overlap_audio( [..] )\n",
    "    elif output_type_i == 4: # syn + uncut\n",
    "        chosen_audio = repeat_audio( [..] )       \n",
    "    elif output_type_i == 5: # syn + overlap\n",
    "        chosen_audio = overlap_audio( [..] )\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    plt.subplot(212)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s[0:170], np.real(h[int(len(H)):int(len(H))+170]))\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Graphische Darstellung\n",
    "    plt.subplot(221)\n",
    "    plt.title('Amplitudenfrequenzgang des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Frequenz [Hz]')\n",
    "    plt.ylabel('$log|H(e^{jΩ})|$')\n",
    "    plt.plot(fH_Hz, np.log(np.abs(H)))\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plt.title('Polverteilung des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Real')\n",
    "    plt.ylabel('Image')\n",
    "    theta = np.arange(0, 2*np.pi, 0.01)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), c='r', lw=0.2)  \n",
    "    plt.plot(np.real(p), np.imag(p), 'x')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.gcf().set_size_inches(15, 4)\n",
    "    \n",
    "    plt.subplot(223)\n",
    "    plt.title('Impulsantwort des Synthesefilters mit Ordnung=%d' %N_i)\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('h [t]')\n",
    "    plt.plot(t_s, np.real(h))\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plt.title('Ausschnitt des Audiosignals')\n",
    "    plt.xlabel('Zeit [s]')\n",
    "    plt.ylabel('x [t]')\n",
    "    plt.plot(t_s, chosen_audio[:512])\n",
    "    plt.gcf().set_size_inches(20, 10)\n",
    "    plt.show()\n",
    "\n",
    "    # Audioausgabe\n",
    "    if output_sound_i == True:\n",
    "        play_audio(chosen_audio,fs_Hz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "Dies ist nur eine rudimentäre Umsetzung eines synthetisierten Vokaltrakts, für die es diverse Verbesserungsmethoden gibt. Außerdem fällt direkt auf, dass sich die Grundfrequenz des synthetisierten Vokals (bzw. der \"Pitch\") ändert, wenn sich die Fensterlänge der überlappten Fenster ändert. Eine Alternative dazu bietet zum Beispiel der sog. PSOLA Algorithmus (\"Pitch Synchronous Overlap Add\"), mit dem das Audiosignal so manipuliert werden kann, dass die Aussprache schneller ist, aber die Grundfrequenz gleich bleibt. Für Interessierte wird auf diese Methoden in der Vorlesung \"Sprachsynthese\" im 9. Semester näher eingegangen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "1. [Emflazie](https://en.wikipedia.org/wiki/Source%E2%80%93filter_model#/media/File:Source-filter_model_diagram.svg), [adaptivedigital](https://www.adaptivedigital.com/lpc/)   \n",
    "2. [Linear predictive coding](https://en.wikipedia.org/wiki/Linear_predictive_coding)\n",
    "3. [Introduction - Linear predictive coding](http://support.ircam.fr/docs/AudioSculpt/3.0/co/LPC.html)  \n",
    "---\n",
    "<div>Notebook erstellt von <a href=\"mailto:arne-lukas.fietkau@tu-dresden.de?Subject=Frage%20zu%20Jupyter%20Notebook%201.2%20IIR%20Filterentwurf\" target=\"_top\">Arne-Lukas Fietkau</a>, Yifei Li  und Christoph Wagner </div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
